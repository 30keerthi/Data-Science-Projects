{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# APPROACH\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mUysYVRXw8Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Load all lib and datas\n",
        "* Data preparation and data transformation\n",
        "* View images\n",
        "* Model\n",
        "     - Neural network(mlp,dense,fcn)\n",
        "     -Sklearn\n",
        "     -tensor flow\n",
        "\n"
      ],
      "metadata": {
        "id": "wilScAqa3SB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING THE LIBRARIES\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6V81fYxgATUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n"
      ],
      "metadata": {
        "id": "xvc8F6Ri3iJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING AND AUDITING THE DATA\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EYV-6d7bAbo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAIN DATA**"
      ],
      "metadata": {
        "id": "j_iLy4DwxKkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('/content/fashion-mnist_train.csv')"
      ],
      "metadata": {
        "id": "PEMQsl_13uhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "oR8YOPat35JT",
        "outputId": "e11935be-8e3b-4d12-8a9d-9661517c444b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0      2       0       0       0       0       0       0       0       0   \n",
              "1      9       0       0       0       0       0       0       0       0   \n",
              "2      6       0       0       0       0       0       0       0       5   \n",
              "3      0       0       0       0       1       2       0       0       0   \n",
              "4      3       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "1       0  ...         0         0         0         0         0         0   \n",
              "2       0  ...         0         0         0        30        43         0   \n",
              "3       0  ...         3         0         0         0         0         1   \n",
              "4       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-1a7dd467-1f62-407c-af2c-96733055f43f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a7dd467-1f62-407c-af2c-96733055f43f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-751898b2-877a-4dac-bba3-a49830bb79f0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-751898b2-877a-4dac-bba3-a49830bb79f0')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-751898b2-877a-4dac-bba3-a49830bb79f0 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a7dd467-1f62-407c-af2c-96733055f43f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a7dd467-1f62-407c-af2c-96733055f43f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDyiws8V3-f_",
        "outputId": "113b5329-e72b-459e-9281-86f79246587a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST DATA**"
      ],
      "metadata": {
        "id": "eoBc4qcMApIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=pd.read_csv('/content/fashion-mnist_test.csv')"
      ],
      "metadata": {
        "id": "tb3599lA4Rdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwdNhmk04NCO",
        "outputId": "d92761bf-55c4-40cb-f848-bd79a776e2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN AND TEST DATA SPLITTING\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YDGI-ElSBjUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=train_data.iloc[:,1:]\n",
        "y=train_data['label']\n"
      ],
      "metadata": {
        "id": "uF7iPN975E-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "nk-Ng_yw5pqU",
        "outputId": "7d128747-73a1-4c43-a42b-511799bae57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
              "0       0       0       0       0       0       0       0       0       0   \n",
              "1       0       0       0       0       0       0       0       0       0   \n",
              "2       0       0       0       0       0       0       0       5       0   \n",
              "3       0       0       0       1       2       0       0       0       0   \n",
              "4       0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0        0  ...         0         0         0         0         0         0   \n",
              "1        0  ...         0         0         0         0         0         0   \n",
              "2        0  ...         0         0         0        30        43         0   \n",
              "3        0  ...         3         0         0         0         0         1   \n",
              "4        0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9cbd560c-5d46-4a7e-86bb-349ec93cbeb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cbd560c-5d46-4a7e-86bb-349ec93cbeb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ea6963ac-3027-4e1c-ab3e-ac5af5d842cb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea6963ac-3027-4e1c-ab3e-ac5af5d842cb')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ea6963ac-3027-4e1c-ab3e-ac5af5d842cb button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9cbd560c-5d46-4a7e-86bb-349ec93cbeb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9cbd560c-5d46-4a7e-86bb-349ec93cbeb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6N7NdHp5txd",
        "outputId": "7827121b-70a7-499e-d270-5befb319d2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    9\n",
              "2    6\n",
              "3    0\n",
              "4    3\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=123)"
      ],
      "metadata": {
        "id": "49dJTOXH52jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxjdqu_X537j",
        "outputId": "19c735f6-3ce7-410d-c43a-552efcb8c7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 784)\n",
            "(12000, 784)\n",
            "(48000,)\n",
            "(12000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING MODEL\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "16mzTRQpxiyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SINGLE LAYER PERCEPTRON"
      ],
      "metadata": {
        "id": "PuKeh_GI_wS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slp=Perceptron(verbose=1)\n",
        "slp.fit(x_train,y_train)\n",
        "preds_slp_train=slp.predict(x_train)\n",
        "preds_slp_test=slp.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKBRQQTM6HhO",
        "outputId": "57974074-09a4-4453-abda-0e1910dcff32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 53717.69, NNZs: 783, Bias: -79.000000, T: 48000, Avg. loss: 300315.256500\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 62354.58, NNZs: 784, Bias: -140.000000, T: 96000, Avg. loss: 297111.988625\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 71210.45, NNZs: 784, Bias: -199.000000, T: 144000, Avg. loss: 286966.761063\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 77926.15, NNZs: 783, Bias: -280.000000, T: 192000, Avg. loss: 294703.786187\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 85439.47, NNZs: 784, Bias: -372.000000, T: 240000, Avg. loss: 294710.997146\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 91029.39, NNZs: 784, Bias: -435.000000, T: 288000, Avg. loss: 287022.618167\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 94805.77, NNZs: 784, Bias: -494.000000, T: 336000, Avg. loss: 287587.723562\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 99334.58, NNZs: 784, Bias: -596.000000, T: 384000, Avg. loss: 288378.091500\n",
            "Total training time: 0.75 seconds.\n",
            "Convergence after 8 epochs took 0.75 seconds\n",
            "-- Epoch 1\n",
            "Norm: 38091.34, NNZs: 777, Bias: -70.000000, T: 48000, Avg. loss: 43514.430417\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45292.24, NNZs: 778, Bias: -141.000000, T: 96000, Avg. loss: 36126.645583\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51398.11, NNZs: 778, Bias: -202.000000, T: 144000, Avg. loss: 35855.865729\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56844.41, NNZs: 777, Bias: -282.000000, T: 192000, Avg. loss: 34326.811146\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 62916.36, NNZs: 779, Bias: -353.000000, T: 240000, Avg. loss: 33784.635250\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 66627.08, NNZs: 781, Bias: -426.000000, T: 288000, Avg. loss: 34062.168875\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 69532.29, NNZs: 781, Bias: -491.000000, T: 336000, Avg. loss: 33921.390437\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 73720.69, NNZs: 784, Bias: -537.000000, T: 384000, Avg. loss: 32591.739083\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 76529.56, NNZs: 784, Bias: -601.000000, T: 432000, Avg. loss: 32353.006167\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 79238.51, NNZs: 784, Bias: -666.000000, T: 480000, Avg. loss: 34130.608833\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 83131.26, NNZs: 783, Bias: -722.000000, T: 528000, Avg. loss: 32855.980542\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 86259.58, NNZs: 784, Bias: -784.000000, T: 576000, Avg. loss: 31263.727271\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 88679.52, NNZs: 784, Bias: -842.000000, T: 624000, Avg. loss: 32446.440792\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 89940.28, NNZs: 784, Bias: -907.000000, T: 672000, Avg. loss: 34348.415375\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 93871.13, NNZs: 784, Bias: -965.000000, T: 720000, Avg. loss: 32201.770354\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96399.69, NNZs: 783, Bias: -1034.000000, T: 768000, Avg. loss: 31039.769167\n",
            "Total training time: 1.60 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 98253.68, NNZs: 784, Bias: -1101.000000, T: 816000, Avg. loss: 32361.757917\n",
            "Total training time: 1.69 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 101009.66, NNZs: 784, Bias: -1157.000000, T: 864000, Avg. loss: 30413.602146\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 103008.95, NNZs: 784, Bias: -1218.000000, T: 912000, Avg. loss: 31798.728729\n",
            "Total training time: 1.90 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 104293.88, NNZs: 784, Bias: -1275.000000, T: 960000, Avg. loss: 31745.055833\n",
            "Total training time: 2.00 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 106925.65, NNZs: 783, Bias: -1331.000000, T: 1008000, Avg. loss: 31471.465167\n",
            "Total training time: 2.10 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 108444.84, NNZs: 784, Bias: -1394.000000, T: 1056000, Avg. loss: 31004.387563\n",
            "Total training time: 2.20 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 110473.61, NNZs: 784, Bias: -1457.000000, T: 1104000, Avg. loss: 32556.966917\n",
            "Total training time: 2.30 seconds.\n",
            "Convergence after 23 epochs took 2.30 seconds\n",
            "-- Epoch 1\n",
            "Norm: 67779.83, NNZs: 784, Bias: -170.000000, T: 48000, Avg. loss: 542180.524667\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 82382.37, NNZs: 784, Bias: -325.000000, T: 96000, Avg. loss: 536909.345625\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 92343.08, NNZs: 784, Bias: -493.000000, T: 144000, Avg. loss: 540416.001875\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 99658.55, NNZs: 784, Bias: -664.000000, T: 192000, Avg. loss: 523864.613104\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 108184.30, NNZs: 784, Bias: -831.000000, T: 240000, Avg. loss: 522317.803625\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 112636.58, NNZs: 784, Bias: -1002.000000, T: 288000, Avg. loss: 528949.705896\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 118516.43, NNZs: 784, Bias: -1160.000000, T: 336000, Avg. loss: 511366.627000\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 123940.19, NNZs: 784, Bias: -1337.000000, T: 384000, Avg. loss: 518012.169979\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 126574.80, NNZs: 784, Bias: -1507.000000, T: 432000, Avg. loss: 524649.755188\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 130042.55, NNZs: 784, Bias: -1675.000000, T: 480000, Avg. loss: 521151.426021\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 134918.10, NNZs: 784, Bias: -1825.000000, T: 528000, Avg. loss: 521569.044875\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 138724.17, NNZs: 784, Bias: -1992.000000, T: 576000, Avg. loss: 511646.776812\n",
            "Total training time: 1.08 seconds.\n",
            "Convergence after 12 epochs took 1.08 seconds\n",
            "-- Epoch 1\n",
            "Norm: 54977.98, NNZs: 773, Bias: -73.000000, T: 48000, Avg. loss: 231783.552333\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 68506.35, NNZs: 773, Bias: -133.000000, T: 96000, Avg. loss: 221306.577646\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 77538.86, NNZs: 783, Bias: -200.000000, T: 144000, Avg. loss: 222949.288208\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 87038.36, NNZs: 784, Bias: -274.000000, T: 192000, Avg. loss: 213488.147729\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 92342.09, NNZs: 784, Bias: -324.000000, T: 240000, Avg. loss: 211846.146083\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 98312.22, NNZs: 784, Bias: -373.000000, T: 288000, Avg. loss: 217146.070854\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 103831.51, NNZs: 784, Bias: -439.000000, T: 336000, Avg. loss: 212903.102604\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 108280.60, NNZs: 784, Bias: -471.000000, T: 384000, Avg. loss: 208766.376771\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 112470.30, NNZs: 783, Bias: -529.000000, T: 432000, Avg. loss: 213933.260542\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 115798.71, NNZs: 784, Bias: -584.000000, T: 480000, Avg. loss: 209824.225688\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 116265.85, NNZs: 784, Bias: -627.000000, T: 528000, Avg. loss: 215179.761396\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 119786.14, NNZs: 784, Bias: -673.000000, T: 576000, Avg. loss: 208025.924125\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 122385.46, NNZs: 784, Bias: -722.000000, T: 624000, Avg. loss: 209232.041250\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 125975.96, NNZs: 784, Bias: -786.000000, T: 672000, Avg. loss: 203612.703229\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 128268.63, NNZs: 784, Bias: -840.000000, T: 720000, Avg. loss: 209753.138917\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 131226.33, NNZs: 784, Bias: -877.000000, T: 768000, Avg. loss: 207961.056500\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 132912.76, NNZs: 784, Bias: -918.000000, T: 816000, Avg. loss: 214026.745813\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 135766.70, NNZs: 784, Bias: -951.000000, T: 864000, Avg. loss: 210373.427333\n",
            "Total training time: 1.58 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 139588.06, NNZs: 784, Bias: -1013.000000, T: 912000, Avg. loss: 210949.791771\n",
            "Total training time: 1.67 seconds.\n",
            "Convergence after 19 epochs took 1.67 seconds\n",
            "-- Epoch 1\n",
            "Norm: 70673.04, NNZs: 780, Bias: -421.000000, T: 48000, Avg. loss: 556874.369875\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 86732.85, NNZs: 784, Bias: -803.000000, T: 96000, Avg. loss: 537256.958562\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 97119.19, NNZs: 784, Bias: -1192.000000, T: 144000, Avg. loss: 543358.548812\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 105669.42, NNZs: 783, Bias: -1572.000000, T: 192000, Avg. loss: 526609.411833\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 112516.39, NNZs: 784, Bias: -1944.000000, T: 240000, Avg. loss: 532192.466792\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 120885.71, NNZs: 783, Bias: -2323.000000, T: 288000, Avg. loss: 526682.901500\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 126851.96, NNZs: 784, Bias: -2693.000000, T: 336000, Avg. loss: 532620.001771\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 130087.64, NNZs: 784, Bias: -3043.000000, T: 384000, Avg. loss: 543251.812333\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 134270.40, NNZs: 783, Bias: -3406.000000, T: 432000, Avg. loss: 522916.356875\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 138058.84, NNZs: 784, Bias: -3770.000000, T: 480000, Avg. loss: 533896.679083\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 142886.81, NNZs: 783, Bias: -4104.000000, T: 528000, Avg. loss: 522112.993292\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 146012.42, NNZs: 784, Bias: -4450.000000, T: 576000, Avg. loss: 520399.667000\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 150694.19, NNZs: 784, Bias: -4807.000000, T: 624000, Avg. loss: 530306.881354\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 153438.38, NNZs: 784, Bias: -5185.000000, T: 672000, Avg. loss: 530360.358292\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 157794.15, NNZs: 784, Bias: -5530.000000, T: 720000, Avg. loss: 528252.885083\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 159771.65, NNZs: 784, Bias: -5896.000000, T: 768000, Avg. loss: 531137.461938\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 162960.26, NNZs: 784, Bias: -6235.000000, T: 816000, Avg. loss: 521432.954563\n",
            "Total training time: 1.54 seconds.\n",
            "Convergence after 17 epochs took 1.54 seconds\n",
            "-- Epoch 1\n",
            "Norm: 53582.19, NNZs: 778, Bias: 346.000000, T: 48000, Avg. loss: 87967.139187\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66185.85, NNZs: 779, Bias: 601.000000, T: 96000, Avg. loss: 78210.861146\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 76006.33, NNZs: 779, Bias: 843.000000, T: 144000, Avg. loss: 73361.486479\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 82470.59, NNZs: 780, Bias: 1065.000000, T: 192000, Avg. loss: 70017.546875\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88816.45, NNZs: 781, Bias: 1294.000000, T: 240000, Avg. loss: 71897.957042\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 93365.67, NNZs: 781, Bias: 1492.000000, T: 288000, Avg. loss: 69309.247083\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 98703.55, NNZs: 781, Bias: 1703.000000, T: 336000, Avg. loss: 68026.834896\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 104362.22, NNZs: 781, Bias: 1923.000000, T: 384000, Avg. loss: 69026.689958\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 107866.73, NNZs: 781, Bias: 2127.000000, T: 432000, Avg. loss: 68587.370688\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 112855.75, NNZs: 781, Bias: 2337.000000, T: 480000, Avg. loss: 67144.819646\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 115676.90, NNZs: 781, Bias: 2535.000000, T: 528000, Avg. loss: 68286.954042\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 118933.06, NNZs: 781, Bias: 2723.000000, T: 576000, Avg. loss: 65099.952042\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 121668.02, NNZs: 781, Bias: 2907.000000, T: 624000, Avg. loss: 66088.419083\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 124469.04, NNZs: 781, Bias: 3099.000000, T: 672000, Avg. loss: 65930.117771\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 126583.21, NNZs: 780, Bias: 3288.000000, T: 720000, Avg. loss: 66631.051938\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 129088.27, NNZs: 781, Bias: 3468.000000, T: 768000, Avg. loss: 66528.769062\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 133143.38, NNZs: 781, Bias: 3666.000000, T: 816000, Avg. loss: 63856.068542\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 134698.38, NNZs: 783, Bias: 3845.000000, T: 864000, Avg. loss: 65441.601104\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 136756.93, NNZs: 783, Bias: 4031.000000, T: 912000, Avg. loss: 65606.519854\n",
            "Total training time: 1.64 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 139761.28, NNZs: 783, Bias: 4222.000000, T: 960000, Avg. loss: 66793.226667\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 142231.20, NNZs: 783, Bias: 4414.000000, T: 1008000, Avg. loss: 64725.941333\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 143346.77, NNZs: 783, Bias: 4597.000000, T: 1056000, Avg. loss: 66152.252063\n",
            "Total training time: 1.90 seconds.\n",
            "Convergence after 22 epochs took 1.90 seconds\n",
            "-- Epoch 1\n",
            "Norm: 70826.39, NNZs: 783, Bias: -59.000000, T: 48000, Avg. loss: 659641.824479\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 82445.85, NNZs: 784, Bias: -98.000000, T: 96000, Avg. loss: 643065.271375\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 91822.60, NNZs: 784, Bias: -147.000000, T: 144000, Avg. loss: 645077.086063\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 96692.78, NNZs: 784, Bias: -221.000000, T: 192000, Avg. loss: 640183.589167\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 103181.03, NNZs: 784, Bias: -288.000000, T: 240000, Avg. loss: 646379.752854\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 107621.49, NNZs: 784, Bias: -372.000000, T: 288000, Avg. loss: 637380.011208\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 112669.82, NNZs: 784, Bias: -449.000000, T: 336000, Avg. loss: 637323.265000\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 116305.75, NNZs: 783, Bias: -498.000000, T: 384000, Avg. loss: 647063.494833\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 121421.25, NNZs: 784, Bias: -554.000000, T: 432000, Avg. loss: 639884.563125\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 124831.39, NNZs: 784, Bias: -608.000000, T: 480000, Avg. loss: 643182.198500\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 128944.04, NNZs: 784, Bias: -651.000000, T: 528000, Avg. loss: 648192.827042\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 132749.53, NNZs: 784, Bias: -717.000000, T: 576000, Avg. loss: 638628.586813\n",
            "Total training time: 1.10 seconds.\n",
            "Convergence after 12 epochs took 1.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 47616.50, NNZs: 769, Bias: -133.000000, T: 48000, Avg. loss: 89025.468833\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 59360.55, NNZs: 774, Bias: -220.000000, T: 96000, Avg. loss: 78202.772938\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68362.09, NNZs: 773, Bias: -293.000000, T: 144000, Avg. loss: 72800.550937\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 74708.43, NNZs: 774, Bias: -377.000000, T: 192000, Avg. loss: 75057.573188\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 79345.55, NNZs: 774, Bias: -452.000000, T: 240000, Avg. loss: 73128.662417\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 84476.89, NNZs: 774, Bias: -516.000000, T: 288000, Avg. loss: 72286.131917\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90029.74, NNZs: 774, Bias: -577.000000, T: 336000, Avg. loss: 73809.629208\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 93720.26, NNZs: 774, Bias: -645.000000, T: 384000, Avg. loss: 75572.145021\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 96610.31, NNZs: 773, Bias: -705.000000, T: 432000, Avg. loss: 73064.647437\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 99248.23, NNZs: 774, Bias: -772.000000, T: 480000, Avg. loss: 72095.103458\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 102030.90, NNZs: 774, Bias: -847.000000, T: 528000, Avg. loss: 72736.635771\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 104404.77, NNZs: 775, Bias: -914.000000, T: 576000, Avg. loss: 72831.641896\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 107519.46, NNZs: 775, Bias: -978.000000, T: 624000, Avg. loss: 73545.790708\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 109994.86, NNZs: 775, Bias: -1028.000000, T: 672000, Avg. loss: 72003.464833\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 113301.26, NNZs: 775, Bias: -1098.000000, T: 720000, Avg. loss: 71728.227229\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 114995.06, NNZs: 774, Bias: -1157.000000, T: 768000, Avg. loss: 71264.344000\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 117970.92, NNZs: 775, Bias: -1218.000000, T: 816000, Avg. loss: 74463.267979\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 119853.14, NNZs: 775, Bias: -1284.000000, T: 864000, Avg. loss: 74333.102458\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 121887.20, NNZs: 775, Bias: -1342.000000, T: 912000, Avg. loss: 72254.656937\n",
            "Total training time: 1.65 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 123849.44, NNZs: 775, Bias: -1400.000000, T: 960000, Avg. loss: 70648.492000\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 125058.08, NNZs: 775, Bias: -1450.000000, T: 1008000, Avg. loss: 70199.495583\n",
            "Total training time: 1.82 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 127555.21, NNZs: 775, Bias: -1516.000000, T: 1056000, Avg. loss: 69184.363396\n",
            "Total training time: 1.90 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 129832.76, NNZs: 774, Bias: -1580.000000, T: 1104000, Avg. loss: 72319.569146\n",
            "Total training time: 1.99 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 132089.07, NNZs: 775, Bias: -1643.000000, T: 1152000, Avg. loss: 71165.577771\n",
            "Total training time: 2.07 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 134023.53, NNZs: 775, Bias: -1712.000000, T: 1200000, Avg. loss: 70961.950687\n",
            "Total training time: 2.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 135539.29, NNZs: 775, Bias: -1778.000000, T: 1248000, Avg. loss: 72241.803563\n",
            "Total training time: 2.25 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 137748.61, NNZs: 775, Bias: -1831.000000, T: 1296000, Avg. loss: 69579.541062\n",
            "Total training time: 2.33 seconds.\n",
            "Convergence after 27 epochs took 2.33 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60127.31, NNZs: 784, Bias: -291.000000, T: 48000, Avg. loss: 139104.052229\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 70483.44, NNZs: 784, Bias: -526.000000, T: 96000, Avg. loss: 126850.633667\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 80260.98, NNZs: 784, Bias: -746.000000, T: 144000, Avg. loss: 124331.255125\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 89237.03, NNZs: 784, Bias: -956.000000, T: 192000, Avg. loss: 120136.181313\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 94654.48, NNZs: 784, Bias: -1166.000000, T: 240000, Avg. loss: 124473.521917\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 100208.76, NNZs: 784, Bias: -1382.000000, T: 288000, Avg. loss: 122536.864250\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 105122.35, NNZs: 784, Bias: -1578.000000, T: 336000, Avg. loss: 117000.323542\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 109122.46, NNZs: 784, Bias: -1790.000000, T: 384000, Avg. loss: 121000.219542\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 113296.02, NNZs: 784, Bias: -1991.000000, T: 432000, Avg. loss: 119343.806500\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 117713.42, NNZs: 784, Bias: -2197.000000, T: 480000, Avg. loss: 121446.022625\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 120247.54, NNZs: 783, Bias: -2401.000000, T: 528000, Avg. loss: 119995.106958\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 124308.72, NNZs: 784, Bias: -2596.000000, T: 576000, Avg. loss: 117234.227250\n",
            "Total training time: 1.19 seconds.\n",
            "Convergence after 12 epochs took 1.19 seconds\n",
            "-- Epoch 1\n",
            "Norm: 59411.81, NNZs: 781, Bias: -334.000000, T: 48000, Avg. loss: 106320.466458\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 74106.02, NNZs: 782, Bias: -573.000000, T: 96000, Avg. loss: 89115.764500\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84969.48, NNZs: 782, Bias: -801.000000, T: 144000, Avg. loss: 85000.023125\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 92161.19, NNZs: 782, Bias: -1014.000000, T: 192000, Avg. loss: 85625.842458\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 98424.36, NNZs: 782, Bias: -1224.000000, T: 240000, Avg. loss: 84000.081063\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 105705.63, NNZs: 782, Bias: -1431.000000, T: 288000, Avg. loss: 83643.485292\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 110600.26, NNZs: 782, Bias: -1627.000000, T: 336000, Avg. loss: 81254.552375\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 115924.63, NNZs: 782, Bias: -1828.000000, T: 384000, Avg. loss: 79558.628021\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 119615.93, NNZs: 782, Bias: -2021.000000, T: 432000, Avg. loss: 83142.525667\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 121972.68, NNZs: 782, Bias: -2204.000000, T: 480000, Avg. loss: 80843.622271\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 126217.04, NNZs: 782, Bias: -2404.000000, T: 528000, Avg. loss: 83039.488458\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 129896.43, NNZs: 782, Bias: -2581.000000, T: 576000, Avg. loss: 78312.150229\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 133274.75, NNZs: 782, Bias: -2764.000000, T: 624000, Avg. loss: 78389.553750\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 136076.19, NNZs: 782, Bias: -2951.000000, T: 672000, Avg. loss: 78948.245229\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 138439.59, NNZs: 782, Bias: -3134.000000, T: 720000, Avg. loss: 80737.073042\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 141532.49, NNZs: 781, Bias: -3312.000000, T: 768000, Avg. loss: 77391.356062\n",
            "Total training time: 1.63 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 143143.90, NNZs: 782, Bias: -3500.000000, T: 816000, Avg. loss: 84825.400667\n",
            "Total training time: 1.72 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 145911.08, NNZs: 781, Bias: -3693.000000, T: 864000, Avg. loss: 79542.443521\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 148400.61, NNZs: 782, Bias: -3868.000000, T: 912000, Avg. loss: 81356.809562\n",
            "Total training time: 1.90 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 150902.03, NNZs: 781, Bias: -4052.000000, T: 960000, Avg. loss: 77670.119375\n",
            "Total training time: 1.99 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 153475.57, NNZs: 782, Bias: -4221.000000, T: 1008000, Avg. loss: 78351.765188\n",
            "Total training time: 2.07 seconds.\n",
            "Convergence after 21 epochs took 2.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(actual,predicted):\n",
        "  acc_score=accuracy_score(actual,predicted)\n",
        "  cm_matrix=confusion_matrix(actual,predicted)\n",
        "  clas_rep=classification_report(actual,predicted)\n",
        "  print(\"Accuracy :\",round(acc_score,2))\n",
        "  print(\" \")\n",
        "  print(cm_matrix)\n",
        "  print(\" \")\n",
        "  print(clas_rep)"
      ],
      "metadata": {
        "id": "UbN4vKBy6Zsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_train,preds_slp_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcua8nPs6eXX",
        "outputId": "2cd4edf3-0ac6-41bc-e873-27461075f4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.83\n",
            " \n",
            "[[4178   12  119  189   97    0  198    0   67    0]\n",
            " [  12 4590   23   90   39    0    2    0    3    0]\n",
            " [  62    5 3497   60 1076    0   54    0   34    0]\n",
            " [ 167   45   48 4214  327    0   31    2   16    1]\n",
            " [  13    4  248  105 4426    0   40    1   13    0]\n",
            " [  27    3   42   12   16 3775    9  638  134  120]\n",
            " [ 769    9  731  191 1471    0 1513    1  106    1]\n",
            " [   0    0    1    0    0   13    0 4680   18   61]\n",
            " [  18    1   48   52   80    2   17   22 4533    3]\n",
            " [   7    2    5    1    2   26    1  339    9 4383]]\n",
            " \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.86      0.83      4860\n",
            "           1       0.98      0.96      0.97      4759\n",
            "           2       0.73      0.73      0.73      4788\n",
            "           3       0.86      0.87      0.86      4851\n",
            "           4       0.59      0.91      0.71      4850\n",
            "           5       0.99      0.79      0.88      4776\n",
            "           6       0.81      0.32      0.45      4792\n",
            "           7       0.82      0.98      0.90      4773\n",
            "           8       0.92      0.95      0.93      4776\n",
            "           9       0.96      0.92      0.94      4775\n",
            "\n",
            "    accuracy                           0.83     48000\n",
            "   macro avg       0.85      0.83      0.82     48000\n",
            "weighted avg       0.85      0.83      0.82     48000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE**"
      ],
      "metadata": {
        "id": "Vu_Pi3Q4GEpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TRAIN ACCURACY =0.83"
      ],
      "metadata": {
        "id": "TEwL0AIvGGoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_test,preds_slp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD4y05iE6fPR",
        "outputId": "62faf0fe-eaa9-46e9-8e1c-2fae8afe19af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.81\n",
            " \n",
            "[[ 968    5   29   51   23    1   48    1   14    0]\n",
            " [   5 1176   11   32   13    0    1    0    2    1]\n",
            " [   8    1  872   21  279    0   21    0   10    0]\n",
            " [  55   14   17  958   92    0    7    1    4    1]\n",
            " [   4    1   71   35 1020    0   16    0    3    0]\n",
            " [   5    1   14    1    4  936    5  181   41   36]\n",
            " [ 207    1  199   41  382    0  341    1   35    1]\n",
            " [   0    0    0    0    0    3    0 1204    1   19]\n",
            " [   3    0    6   20   22    1    7   10 1155    0]\n",
            " [   7    0    1    1    0    8    0  104    2 1102]]\n",
            " \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.85      0.81      1140\n",
            "           1       0.98      0.95      0.96      1241\n",
            "           2       0.71      0.72      0.72      1212\n",
            "           3       0.83      0.83      0.83      1149\n",
            "           4       0.56      0.89      0.68      1150\n",
            "           5       0.99      0.76      0.86      1224\n",
            "           6       0.76      0.28      0.41      1208\n",
            "           7       0.80      0.98      0.88      1227\n",
            "           8       0.91      0.94      0.93      1224\n",
            "           9       0.95      0.90      0.92      1225\n",
            "\n",
            "    accuracy                           0.81     12000\n",
            "   macro avg       0.83      0.81      0.80     12000\n",
            "weighted avg       0.83      0.81      0.80     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE**"
      ],
      "metadata": {
        "id": "akfAc7tZGLt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TEST_ACCURACY=0.81"
      ],
      "metadata": {
        "id": "zCChnyx_GN2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MULTILAYER PERCEPTRON\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8Yc_k0v3_j5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp=MLPClassifier(hidden_layer_sizes=(128,128,128),verbose=1,max_iter=250)"
      ],
      "metadata": {
        "id": "iew2Fbiw6vqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(x_train,y_train)\n",
        "preds_mlp_train=mlp.predict(x_train)\n",
        "preds_mlp_test=mlp.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1kZ0Ipm6wrZ",
        "outputId": "3e4c418f-1bd1-46c0-89ff-3be526c402cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.53404323\n",
            "Iteration 2, loss = 0.75304009\n",
            "Iteration 3, loss = 0.56216778\n",
            "Iteration 4, loss = 0.48972986\n",
            "Iteration 5, loss = 0.43547526\n",
            "Iteration 6, loss = 0.39328259\n",
            "Iteration 7, loss = 0.38157588\n",
            "Iteration 8, loss = 0.35679036\n",
            "Iteration 9, loss = 0.34481709\n",
            "Iteration 10, loss = 0.34863870\n",
            "Iteration 11, loss = 0.32890795\n",
            "Iteration 12, loss = 0.32310020\n",
            "Iteration 13, loss = 0.31213367\n",
            "Iteration 14, loss = 0.30777674\n",
            "Iteration 15, loss = 0.31104530\n",
            "Iteration 16, loss = 0.30468497\n",
            "Iteration 17, loss = 0.29485755\n",
            "Iteration 18, loss = 0.29336908\n",
            "Iteration 19, loss = 0.29085002\n",
            "Iteration 20, loss = 0.29309512\n",
            "Iteration 21, loss = 0.29109309\n",
            "Iteration 22, loss = 0.27290077\n",
            "Iteration 23, loss = 0.26990908\n",
            "Iteration 24, loss = 0.27029084\n",
            "Iteration 25, loss = 0.26338999\n",
            "Iteration 26, loss = 0.26286662\n",
            "Iteration 27, loss = 0.26176132\n",
            "Iteration 28, loss = 0.25807350\n",
            "Iteration 29, loss = 0.25883680\n",
            "Iteration 30, loss = 0.25029555\n",
            "Iteration 31, loss = 0.25256983\n",
            "Iteration 32, loss = 0.24139748\n",
            "Iteration 33, loss = 0.23650788\n",
            "Iteration 34, loss = 0.23573688\n",
            "Iteration 35, loss = 0.24492273\n",
            "Iteration 36, loss = 0.22953896\n",
            "Iteration 37, loss = 0.22307458\n",
            "Iteration 38, loss = 0.21838878\n",
            "Iteration 39, loss = 0.21674740\n",
            "Iteration 40, loss = 0.21873296\n",
            "Iteration 41, loss = 0.21059359\n",
            "Iteration 42, loss = 0.21195776\n",
            "Iteration 43, loss = 0.22203027\n",
            "Iteration 44, loss = 0.21097318\n",
            "Iteration 45, loss = 0.20449376\n",
            "Iteration 46, loss = 0.20089408\n",
            "Iteration 47, loss = 0.19635570\n",
            "Iteration 48, loss = 0.20250102\n",
            "Iteration 49, loss = 0.18988349\n",
            "Iteration 50, loss = 0.18577341\n",
            "Iteration 51, loss = 0.18759678\n",
            "Iteration 52, loss = 0.19437218\n",
            "Iteration 53, loss = 0.18149868\n",
            "Iteration 54, loss = 0.18365932\n",
            "Iteration 55, loss = 0.18652147\n",
            "Iteration 56, loss = 0.18468142\n",
            "Iteration 57, loss = 0.18784783\n",
            "Iteration 58, loss = 0.17179824\n",
            "Iteration 59, loss = 0.17673096\n",
            "Iteration 60, loss = 0.16954727\n",
            "Iteration 61, loss = 0.16498963\n",
            "Iteration 62, loss = 0.16213141\n",
            "Iteration 63, loss = 0.16621480\n",
            "Iteration 64, loss = 0.16769115\n",
            "Iteration 65, loss = 0.17159212\n",
            "Iteration 66, loss = 0.16419110\n",
            "Iteration 67, loss = 0.16116334\n",
            "Iteration 68, loss = 0.16006786\n",
            "Iteration 69, loss = 0.15432933\n",
            "Iteration 70, loss = 0.15343595\n",
            "Iteration 71, loss = 0.15868768\n",
            "Iteration 72, loss = 0.15482635\n",
            "Iteration 73, loss = 0.14702327\n",
            "Iteration 74, loss = 0.14770651\n",
            "Iteration 75, loss = 0.14315927\n",
            "Iteration 76, loss = 0.15154827\n",
            "Iteration 77, loss = 0.14960697\n",
            "Iteration 78, loss = 0.14384842\n",
            "Iteration 79, loss = 0.13811935\n",
            "Iteration 80, loss = 0.14150647\n",
            "Iteration 81, loss = 0.15376788\n",
            "Iteration 82, loss = 0.13980486\n",
            "Iteration 83, loss = 0.14603386\n",
            "Iteration 84, loss = 0.13971212\n",
            "Iteration 85, loss = 0.13489234\n",
            "Iteration 86, loss = 0.13302584\n",
            "Iteration 87, loss = 0.12829038\n",
            "Iteration 88, loss = 0.12884701\n",
            "Iteration 89, loss = 0.13555696\n",
            "Iteration 90, loss = 0.12480946\n",
            "Iteration 91, loss = 0.12843413\n",
            "Iteration 92, loss = 0.12978297\n",
            "Iteration 93, loss = 0.13608905\n",
            "Iteration 94, loss = 0.12271231\n",
            "Iteration 95, loss = 0.12121663\n",
            "Iteration 96, loss = 0.13249022\n",
            "Iteration 97, loss = 0.12696104\n",
            "Iteration 98, loss = 0.12406209\n",
            "Iteration 99, loss = 0.12024576\n",
            "Iteration 100, loss = 0.11400193\n",
            "Iteration 101, loss = 0.12515872\n",
            "Iteration 102, loss = 0.12371965\n",
            "Iteration 103, loss = 0.12570907\n",
            "Iteration 104, loss = 0.12077977\n",
            "Iteration 105, loss = 0.10793617\n",
            "Iteration 106, loss = 0.11086850\n",
            "Iteration 107, loss = 0.11898193\n",
            "Iteration 108, loss = 0.12222826\n",
            "Iteration 109, loss = 0.11277496\n",
            "Iteration 110, loss = 0.11584760\n",
            "Iteration 111, loss = 0.10766551\n",
            "Iteration 112, loss = 0.10565072\n",
            "Iteration 113, loss = 0.10618354\n",
            "Iteration 114, loss = 0.10886529\n",
            "Iteration 115, loss = 0.11998598\n",
            "Iteration 116, loss = 0.12301162\n",
            "Iteration 117, loss = 0.11311337\n",
            "Iteration 118, loss = 0.09732641\n",
            "Iteration 119, loss = 0.11671057\n",
            "Iteration 120, loss = 0.10350427\n",
            "Iteration 121, loss = 0.10682565\n",
            "Iteration 122, loss = 0.10426942\n",
            "Iteration 123, loss = 0.10664025\n",
            "Iteration 124, loss = 0.10886054\n",
            "Iteration 125, loss = 0.09788751\n",
            "Iteration 126, loss = 0.09376149\n",
            "Iteration 127, loss = 0.09557989\n",
            "Iteration 128, loss = 0.09081810\n",
            "Iteration 129, loss = 0.09349345\n",
            "Iteration 130, loss = 0.12312781\n",
            "Iteration 131, loss = 0.10580390\n",
            "Iteration 132, loss = 0.10547612\n",
            "Iteration 133, loss = 0.09365120\n",
            "Iteration 134, loss = 0.08457942\n",
            "Iteration 135, loss = 0.08540907\n",
            "Iteration 136, loss = 0.09900132\n",
            "Iteration 137, loss = 0.09208724\n",
            "Iteration 138, loss = 0.09742008\n",
            "Iteration 139, loss = 0.09632684\n",
            "Iteration 140, loss = 0.10393611\n",
            "Iteration 141, loss = 0.10481207\n",
            "Iteration 142, loss = 0.08867219\n",
            "Iteration 143, loss = 0.08952859\n",
            "Iteration 144, loss = 0.09110478\n",
            "Iteration 145, loss = 0.08526166\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE:**"
      ],
      "metadata": {
        "id": "lPn8ENb1C3RL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* BY PROVIDING THE HIDDEN LAYER SIZES AS (128,128,128)\n",
        "\n",
        "* WE CAN SEE THERE IS AROUND 145 ITERATIONS WITH LOSS=0.085.\n",
        "\n",
        "* ALSO WE CAN WITNESS THAT THE TRAINING LOSS DID  NOT IMPROVE MORE THAN TOLEARANCE LEVEL(i.e) tol=0.0001 .\n",
        "\n",
        "* SO THE ITERATION WAS STOPPED BRFORE EXECUTING THE GIVEN MAX_ITER=250."
      ],
      "metadata": {
        "id": "P0RyVaGdC9On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_train,preds_mlp_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrWSbYku62Kk",
        "outputId": "3a70f690-e1ba-4c04-a3cb-c16adf6ac43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.98\n",
            " \n",
            "[[4625    0   13   34    3    0  183    0    2    0]\n",
            " [   0 4747    0    9    2    0    1    0    0    0]\n",
            " [   6    0 4491   10  212    0   67    0    2    0]\n",
            " [   4    1    6 4789   46    1    4    0    0    0]\n",
            " [   2    0   95   41 4657    0   53    0    2    0]\n",
            " [   0    0    0    0    0 4776    0    0    0    0]\n",
            " [ 118    0   93   36  110    0 4435    0    0    0]\n",
            " [   0    0    0    0    0    2    0 4767    1    3]\n",
            " [   1    0    0    2    1    0    0    0 4772    0]\n",
            " [   0    0    0    0    0    1    0   22    0 4752]]\n",
            " \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      4860\n",
            "           1       1.00      1.00      1.00      4759\n",
            "           2       0.96      0.94      0.95      4788\n",
            "           3       0.97      0.99      0.98      4851\n",
            "           4       0.93      0.96      0.94      4850\n",
            "           5       1.00      1.00      1.00      4776\n",
            "           6       0.94      0.93      0.93      4792\n",
            "           7       1.00      1.00      1.00      4773\n",
            "           8       1.00      1.00      1.00      4776\n",
            "           9       1.00      1.00      1.00      4775\n",
            "\n",
            "    accuracy                           0.98     48000\n",
            "   macro avg       0.98      0.98      0.98     48000\n",
            "weighted avg       0.98      0.98      0.98     48000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE**"
      ],
      "metadata": {
        "id": "Z0ygaMa-EnHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   THE TRAIN ACCURACY = 0.98.\n",
        "*   WE CAN CLEARLY SEE  WHICH LABEL CAN BE MORE DIFFICULT TO BE DIFFERENCIATED FROM THE OTHER LABELS.\n",
        "* THIS CAN BE ACHIEVED BY CALCULATING THE f1 SCORES .\n",
        "* FROM THE ABOVE LABELS, WE CAN SAY THAT LABEL 6 AND 4 MAY BE QUITE COMPLEX.\n",
        "\n"
      ],
      "metadata": {
        "id": "vewaFRzAEqDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_test,preds_mlp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlR0ECf266O5",
        "outputId": "24f1b85f-5cb7-4b7c-9693-35e6cc5282c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.89\n",
            " \n",
            "[[ 919    0   20   48    4    0  144    0    5    0]\n",
            " [   2 1210    1   24    1    0    1    0    2    0]\n",
            " [  10    0  966   21  118    0   93    0    4    0]\n",
            " [  16   11    7 1030   52    0   31    0    1    1]\n",
            " [   5    2   85   35  959    0   58    0    6    0]\n",
            " [   1    0    0    0    0 1174    1   25    8   15]\n",
            " [ 121    3  103   29   81    0  860    0   11    0]\n",
            " [   0    0    0    0    0   20    2 1160    4   41]\n",
            " [   6    0    2    6    7    2    9    1 1190    1]\n",
            " [   0    0    0    1    0    6    0   48    1 1169]]\n",
            " \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83      1140\n",
            "           1       0.99      0.98      0.98      1241\n",
            "           2       0.82      0.80      0.81      1212\n",
            "           3       0.86      0.90      0.88      1149\n",
            "           4       0.78      0.83      0.81      1150\n",
            "           5       0.98      0.96      0.97      1224\n",
            "           6       0.72      0.71      0.71      1208\n",
            "           7       0.94      0.95      0.94      1227\n",
            "           8       0.97      0.97      0.97      1224\n",
            "           9       0.95      0.95      0.95      1225\n",
            "\n",
            "    accuracy                           0.89     12000\n",
            "   macro avg       0.89      0.89      0.89     12000\n",
            "weighted avg       0.89      0.89      0.89     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   THE TEST ACCURACY = 0.89.\n",
        "*   WE CAN CLEARLY SEE  WHICH LABEL CAN BE MORE DIFFICULT TO BE DIFFERENCIATED FROM THE OTHER LABELS.\n",
        "* THIS CAN BE ACHIEVED BY CALCULATING THE f1 SCORES .\n",
        "* FROM THE ABOVE LABELS, WE CAN SAY THAT LABEL 6,4 AND 2 MAY BE QUITE COMPLEX.\n"
      ],
      "metadata": {
        "id": "ObrtKVTjFvQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"pixel{}\".format(pixel_num) for pixel_num in range(1,785)]\n",
        "rows_to_examine = 10from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "image_data = np.reshape(train_data[features][rows_to_examine:rows_to_examine+1].to_numpy(), (28,28))\n",
        "plt.imshow(image_data, cmap = 'gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ds9pB8aQBy54",
        "outputId": "5989fe5b-9c85-4d7a-8217-2f59d408160c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79a5b65c9060>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}